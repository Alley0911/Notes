# 超参数
## reg_alpha和reg_lambda
在XGBoost中，reg_alpha和reg_lambda是用于控制模型正则化的超参数。它们在模型训练过程中有助于防止过拟合，并且对模型的性能和复杂度有重要影响。

### reg_alpha (L1 正则化)

- **定义**：reg_alpha参数用于L1正则化（也称为Lasso正则化），它在损失函数中添加了绝对值形式的惩罚项。
- **数学形式**：L1正则化的惩罚项是 \(\alpha \cdot \sum_{i} |w_i|\)，其中\(\alpha\)是reg_alpha，\(w_i\)是模型的权重。
- **影响**：
  - **稀疏性**：L1正则化有助于产生稀疏模型（即一些权重被强制为零），因为绝对值函数在零点处的不连续性会导致部分权重在优化过程中被完全压缩至零，从而产生稀疏性（即有些特征被完全忽略），这使得L1正则化特别适用于特征选择。
  - **复杂度**：增加reg_alpha的值会使模型变得更简单，因为更多的特征权重会被压缩到零。

### reg_lambda (L2 正则化)

- **定义**：reg_lambda参数用于L2正则化（也称为Ridge正则化），它在损失函数中添加了平方形式的惩罚项。
- **数学形式**：L2正则化的惩罚项是 \(\lambda \cdot \sum_{i} w_i^2\)，其中\(\lambda\)是reg_lambda，\(w_i\)是模型的权重。
- **影响**：
  - **平滑性**：L2正则化有助于产生较为平滑的模型，因为它倾向于让所有的权重变得更小但不为零。这种正则化方式则倾向于将权重压缩到较小的值，而不是完全压缩为零。这是因为平方函数在零点处是光滑的，优化过程会使得权重逐渐减小但不容易到达零。结果是所有的特征都会有一些影响力，但这些影响力被均匀地缩小。
  - **复杂度**：增加reg_lambda的值会使模型复杂度降低，因为这限制了权重的大小。

### 为什么 L1 会产生稀疏性而 L2 不会？

- **L1 正则化**：由于绝对值函数在零点处的非连续性，优化过程会在某些情况下将权重压缩到零。这种“稀疏”效果使L1正则化能够自动进行特征选择，忽略那些不重要的特征。
- **L2 正则化**：平方函数在零点处是连续且光滑的，因此权重不会被完全压缩到零，而是会趋向于较小值。这意味着所有特征仍然会有一些影响力，只是影响力会变得很小。

### 总结

- **reg_alpha**（L1正则化）：通过增加特征的稀疏性来减少过拟合。
- **reg_lambda**（L2正则化）：通过减小权重的大小来减少过拟合。