此笔记的内容主要来自以下视频:
1. B站Up主【王木头学科学】关于[交叉熵、KL散度等概念的讲解视频](https://www.bilibili.com/video/BV15V411W7VB/?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=f042d191625f00d83c498fe925998398), 
2. B站Up主【耿直哥】关于[熵](https://www.bilibili.com/video/BV1jk4y1N7W3/?spm_id_from=333.788.top_right_bar_window_history.content.click&vd_source=f042d191625f00d83c498fe925998398)的视频.

> 要想理解**交叉熵**和**KL散度(又叫相对熵)**, 首先需要理解**信息量**, **熵**的概念. 

# 信息量
衡量了一个事件由不确定转换为确定的难度, 是这个事件**不确定性**的度量, 这个事件信息量越大, 它的不确定性越高, 由不确定转换为确定的难度越高. 譬如, 假如国足夺冠的概率很低, 而国足夺冠这个信息的信息量就巨大. 信息量的定义如下:

$$
f(x) = -log_{n}{p(x)} = log_{n}{\frac{1}{p(x)}}
$$

$f(x)$表示事件x的信息量, $p(x)$表示x事件发生的概率(这个概率需要是已知的才能计算信息量), **而概率的倒数$\frac{1}{p(x)}$可以理解为不确定性**, 概率越低不确定越大; 对数运算表示进制转换后不确定性的复杂度, 对数函数的的底数可以是任意数, 当n为2时, 信息量的单位为比特, 可以理解为用多少比特可以表示这个事件的不确定性. 

譬如, 国足夺冠的概率为1/1024, 那么国足夺冠的信息量(当$n=2$)就是$-log_{2}{\frac{1}{1024}}=10 bit$.

# 熵
此处的熵并不是热力学中的熵, 而是**信息论**中的概念. 熵描述了基于某个概率分布的**系统(或者说随机变量)**的平均不确定性, 因此需要提前知道这个系统(或随机变量)的概率分布. 公式如下:
$$
H(X)=-\sum_{i}{P(X_i)}log_b{P(X_i)}
$$

其中$X$代表随机变量, $P(X_i)$代表$X_i$事件发生的概率.

譬如对于投一次骰子的随机事件, 骰子结果这个随机变量$X$($X=1,2,3,4,5,6$, 每个事件的概率都为$\frac{1}{6}$)的熵可以表示为:

$$
H(X)=(-\frac{1}{6}log_2{\frac{1}{6}}) \times 6
$$

# KL散度(相对熵)和交叉熵
KL散度用于比较两个系统熵的差异，计算公式如下：
$$
\begin{align*}
& D_{KL}(P||Q) \\
=& \sum_{i=1}^{m}p_i\cdot(f_{Q}(q_i)-f_P(p_i)) \\
=& \sum_{i=1}^{m}p_i\cdot(-log(q_i)-(-log(p_i))) \\
=& \sum_{i=1}^{m}p_i\cdot(-log(q_i))-pi \cdot(-log(p_i)) 
\end{align*}
$$
上式最后一行的左侧的式子就是$H(P, Q)$的**交叉熵**, 右侧的式子是$H(P)$, 就是随机变量$P$的熵.
上式最后一行的左侧和右侧都是一个≥0的数, 因为概率都是小于等于1. 但是左侧的值一定是≥右式的. 因此, KL散度是一个≥0的数, 只有当P和Q的分布完全相同时才为0(这个定义先记住就好).
**由公式可知, $D_{KL}(P||Q)\not ={D_{KL}(Q||P)}$, 哪个在前面, 哪个就是基准分布**.